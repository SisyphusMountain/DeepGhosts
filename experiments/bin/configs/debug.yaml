seed: 1729

# Model choice
model: "transformer_gcn"
# MLP hyperparams
mlp:
  hidden_layers: [32, 32]
  dropout: 0.0

transformer:
  d_model: 64
  n_heads: 4
  n_blocks: 4
  mlp_expansion_factor: 4
  aggr: "add"
# Hyperparameters
training:
  lr: 5.0e-4
  beta_1: 0.9
  beta_2: 0.999
  eps: 1.0e-8
  weight_decay: 1.0e-3

  warmup_epochs: 10
  epochs: 100
  batch_size: 48
  scheduler: null
  loss_type: "MSE"
  early_stopping: true
  keep_top_k: 1
  patience: 100
  validate_every_n_epoch: 5
data:
  normalize: true
  split_ratio: 0.8
  node:
    in_features: 6
    out_features: 1 # A ghost length per node
wandb:
  project: "deepghosts_test"
  run_name: "debug_transformer_gcn_no_scheduler"
  save_dir: "training_results/debug"
  log_model: true
logging:
  log_plot_every_n_validation_epoch: 50
  measure_parameters_every_n_epochs: 10
  log_window_size: 10
  log_step: 10
  log_sigma: 2.0
  log_num_bins: 20
  log_violin_width: 1.5
  fig_size: 12
  y_lim: 12