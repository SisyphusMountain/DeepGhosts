{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b241741-5ba8-4327-a62d-a81641222b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "import einops\n",
    "\n",
    "class VanillaAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.n_heads = n_heads\n",
    "        self.scaling = (d_model//n_heads)**(-0.5)\n",
    "        self.final_linear = nn.Linear(d_model, d_model)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, edge_index=None, edge_attr=None, parenthood=None, batch=None):\n",
    "        x, mask = to_dense_batch(x, batch) # x has shape [batch_size, num_nodes, d_model]\n",
    "        attn_mask = mask.unsqueeze(1) * mask.unsqueeze(2)\n",
    "        float_attn_mask = ((~attn_mask)*(-1.0e9)).unsqueeze(1)\n",
    "        keys = self.W_K(x)\n",
    "        queries = self.W_Q(x)\n",
    "        values = self.W_V(x)\n",
    "        keys = einops.rearrange(keys, \"b n (h x) -> b n h x\", h = self.n_heads)\n",
    "        queries = einops.rearrange(queries, \"b n (h x) -> b n h x\", h = self.n_heads)\n",
    "        values = einops.rearrange(values, \"b n (h x) -> b n h x\", h = self.n_heads)\n",
    "        attn_coefficients = torch.einsum(\"bihx, bjhx  -> bhij\",keys, queries)*self.scaling\n",
    "        attn_coefficients += float_attn_mask\n",
    "        softmaxed_attn_coefficients = torch.softmax(attn_coefficients, dim = -1)\n",
    "        softmaxed_attn_coefficients = self.attn_dropout(softmaxed_attn_coefficients)\n",
    "        computed_values = torch.einsum(\"bhij, bjhx-> bihx\", softmaxed_attn_coefficients, values)\n",
    "        concatenated_values = einops.rearrange(computed_values, \"b i h x ->b i (h x)\").contiguous()\n",
    "        out_dense = self.final_linear(concatenated_values)\n",
    "        out = out_dense[mask]\n",
    "        return self.resid_dropout(out)\n",
    "    \n",
    "att = VanillaAttention(d_model=8, n_heads=2, dropout=0.0)\n",
    "x = torch.zeros(5,8)\n",
    "x[0] = 1\n",
    "x[1] = 2\n",
    "x[2] = 3\n",
    "x[3] = 4\n",
    "x[4] = 5\n",
    "x = torch.randn_like(x)\n",
    "batch = torch.tensor([0,0,0,1,1])\n",
    "test_1 = att(x=x, batch=batch)\n",
    "x[0] = 1\n",
    "test_2 = att(x=x, batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_gcn import TransformerGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch_geometric.loader import DataLoader\n",
    "with open(\"/media/enzo/Stockage/Output_general/dataset_3/dataset.pkl\", \"rb\") as f:\n",
    "    full_dataset = pickle.load(f)\n",
    "\n",
    "dataloader = DataLoader(full_dataset, batch_size=2, shuffle=False)\n",
    "item = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerGCN(node_in_features=6,\n",
    "                             d_model=32,\n",
    "                             n_heads=2,\n",
    "                             mlp_expansion_factor=2,\n",
    "                             n_blocks=2,\n",
    "                             aggr=\"add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = item[\"node\"].x\n",
    "edge_index = item[\"node\", \"sends_gene_to\", \"node\"].edge_index\n",
    "edge_attr = item[\"node\", \"sends_gene_to\", \"node\"].edge_attr\n",
    "batch = item[\"node\"].batch\n",
    "parenthood = item[\"node\", \"is_parent_of\", \"node\"].edge_index\n",
    "result = transformer(x, edge_index, edge_attr, parenthood, batch)\n",
    "l = result.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/enzo/Documents/git/WP1/DeepGhosts/experiments/batch.pkl\", \"rb\") as f:\n",
    "    batch = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/enzo/Documents/git/WP1/DeepGhosts/experiments/batch_node.pkl\", \"rb\") as f:\n",
    "    batch_node = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/enzo/Documents/git/WP1/DeepGhosts/experiments/batch_sgt.pkl\", \"rb\") as f:\n",
    "    batch_sgt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/enzo/Documents/git/WP1/DeepGhosts/experiments/batch_isp.pkl\", \"rb\") as f:\n",
    "    batch_isp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch_node.x\n",
    "edge_index = batch_sgt.edge_index\n",
    "edge_attr = batch_sgt.edge_attr\n",
    "batch = batch_node.batch\n",
    "parenthood = batch_isp.edge_index\n",
    "result = transformer(x, edge_index, edge_attr, parenthood, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = transformer.embedding(x, edge_index, edge_attr, parenthood, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerMPNN(\n",
       "  (transformer_block): VanillaTransformerBlock(\n",
       "    (attention): VanillaAttention(\n",
       "      (W_K): Linear(in_features=32, out_features=32, bias=False)\n",
       "      (W_Q): Linear(in_features=32, out_features=32, bias=False)\n",
       "      (W_V): Linear(in_features=32, out_features=32, bias=False)\n",
       "      (final_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    )\n",
       "    (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (message_passing): GCN(\n",
       "    (layer): GCNConv(32, 32)\n",
       "  )\n",
       "  (aggregation): Linear(in_features=64, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.transformer_blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6674, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.transformer_blocks[0].transformer_block.attention(emb, batch=batch).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8480,  1.1060, -1.2063,  ..., -1.1965,  0.4992,  0.5279],\n",
       "        [ 1.2813,  0.7959, -0.1550,  ..., -1.0034, -0.2451, -0.8826],\n",
       "        [ 1.3536,  1.1594, -0.0518,  ..., -1.2289, -0.2262, -0.7620],\n",
       "        ...,\n",
       "        [ 0.5683, -0.4899, -0.4450,  ...,  0.4125,  0.4441, -0.3414],\n",
       "        [ 0.5261, -0.4859, -0.3067,  ...,  0.3840,  0.3767, -0.4991],\n",
       "        [ 0.5552, -0.4558, -0.3919,  ...,  0.3860,  0.4381, -0.4166]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.transformer_blocks[0].transformer_block(emb, batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcnconv = transformer.transformer_blocks[0].message_passing.layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_inv = edge_index[[1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,  ..., 108, 108, 118],\n",
       "        [ 15,  24,  25,  ...,  84, 105,  40]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.6.1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_geometric\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "torch_geometric.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNConv(32, 32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcnconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([119, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcnconv(emb, edge_index_inv, edge_attr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.transformer_blocks[0].message_passing(emb, edge_index, edge_attr).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0646],\n",
       "        [-1.0530],\n",
       "        [-1.0879],\n",
       "        ...,\n",
       "        [-0.1701],\n",
       "        [-0.1701],\n",
       "        [-0.1584]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "        ...,\n",
       "        [-1.3901,  0.0060,  0.4786,  ...,  0.0300,  0.6529,  0.2032],\n",
       "        [-1.2956,  0.0104,  0.4279,  ..., -0.1605,  0.4879,  0.1784],\n",
       "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.transformer_blocks[0](emb, edge_index, edge_attr, parenthood, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(x, edge_index, edge_attr, parenthood, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.node_embedding.weight tensor(616.2884) tensor(3.2843, grad_fn=<LinalgVectorNormBackward0>)\n",
      "embedding.node_embedding.bias tensor(23.7653) tensor(1.5283, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.aggregation.weight tensor(1162.4742) tensor(3.2305, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.aggregation.bias tensor(19.6875) tensor(0.4217, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.transformer_block.attention.W_K.weight tensor(0.9841) tensor(3.2997, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.transformer_block.attention.W_Q.weight tensor(0.8953) tensor(3.3082, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.transformer_block.attention.W_V.weight tensor(27.8781) tensor(3.2212, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.transformer_block.attention.final_linear.weight tensor(25.6120) tensor(3.2518, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.transformer_block.attention.final_linear.bias tensor(8.5605) tensor(0.5144, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.transformer_block.mlp.0.weight tensor(18.3147) tensor(4.6474, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.transformer_block.mlp.0.bias tensor(3.4240) tensor(0.7859, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.transformer_block.mlp.2.weight tensor(24.9084) tensor(3.2407, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.transformer_block.mlp.2.bias tensor(8.4903) tensor(0.3852, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.transformer_block.layer_norm_1.weight tensor(3.3468) tensor(5.6579, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.transformer_block.layer_norm_1.bias tensor(3.6109) tensor(0.0057, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.transformer_block.layer_norm_2.weight tensor(1.9632) tensor(5.6554, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.transformer_block.layer_norm_2.bias tensor(2.0363) tensor(0.0057, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.message_passing.layer.bias tensor(7.1534) tensor(0.0057, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.0.message_passing.layer.lin.weight tensor(316.9000) tensor(5.7294, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.aggregation.weight tensor(1156.7705) tensor(3.2420, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.aggregation.bias tensor(16.3188) tensor(0.4396, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.transformer_block.attention.W_K.weight tensor(0.9957) tensor(3.2843, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.transformer_block.attention.W_Q.weight tensor(0.9600) tensor(3.2366, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.transformer_block.attention.W_V.weight tensor(23.1927) tensor(3.2938, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.transformer_block.attention.final_linear.weight tensor(27.0535) tensor(3.2831, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.transformer_block.attention.final_linear.bias tensor(7.8159) tensor(0.5912, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.transformer_block.mlp.0.weight tensor(15.2588) tensor(4.5473, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.transformer_block.mlp.0.bias tensor(2.8526) tensor(0.8839, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.transformer_block.mlp.2.weight tensor(19.8992) tensor(3.3187, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.transformer_block.mlp.2.bias tensor(7.8346) tensor(0.3896, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.transformer_block.layer_norm_1.weight tensor(2.1529) tensor(5.6579, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.transformer_block.layer_norm_1.bias tensor(2.1524) tensor(0.0057, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.transformer_block.layer_norm_2.weight tensor(1.3962) tensor(5.6579, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.transformer_block.layer_norm_2.bias tensor(1.5157) tensor(0.0057, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.message_passing.layer.bias tensor(6.3348) tensor(0.0057, grad_fn=<LinalgVectorNormBackward0>)\n",
      "transformer_blocks.1.message_passing.layer.lin.weight tensor(347.3675) tensor(5.6687, grad_fn=<LinalgVectorNormBackward0>)\n",
      "unembedding.linear.weight tensor(1336.7153) tensor(0.6414, grad_fn=<LinalgVectorNormBackward0>)\n",
      "unembedding.linear.bias tensor(25.3943) tensor(0.0707, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# see parameter gradients\n",
    "for name, param in transformer.named_parameters():\n",
    "    print(name, param.grad.norm(), param.norm())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepGhosts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
